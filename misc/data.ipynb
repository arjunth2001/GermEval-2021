{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0575a4a1-9b51-4d8c-8a45-b0ff78dbfde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecc42798-ee9b-4197-9320-138eeac96e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>Sub1_Toxic</th>\n",
       "      <th>Sub2_Engaging</th>\n",
       "      <th>Sub3_FactClaiming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ziemlich traurig diese Kommentare zu lesen. Ih...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sag ich doch, wir befeuern den Klimawandel. Ra...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Schublade auf, Schublade zu. Zu mehr Denkleist...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Dummerweise haben wir in der EU und in der USA...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>\"So lange Gewinnmaximierung Vorrang hat, wird ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3239</th>\n",
       "      <td>3240</td>\n",
       "      <td>Hier mal eine Info. Flüchtlinge werden 10 km v...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3240</th>\n",
       "      <td>3241</td>\n",
       "      <td>@USER.aha .Mal abwarten kommt bei uns auch .Fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3241</th>\n",
       "      <td>3242</td>\n",
       "      <td>@USER .So ist es</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3242</th>\n",
       "      <td>3243</td>\n",
       "      <td>@USER .Die warten da</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243</th>\n",
       "      <td>3244</td>\n",
       "      <td>@USER .Das bekommen die gesagt wie sich verhal...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3244 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id                                       comment_text  \\\n",
       "0              1  Ziemlich traurig diese Kommentare zu lesen. Ih...   \n",
       "1              2  Sag ich doch, wir befeuern den Klimawandel. Ra...   \n",
       "2              3  Schublade auf, Schublade zu. Zu mehr Denkleist...   \n",
       "3              4  Dummerweise haben wir in der EU und in der USA...   \n",
       "4              5  \"So lange Gewinnmaximierung Vorrang hat, wird ...   \n",
       "...          ...                                                ...   \n",
       "3239        3240  Hier mal eine Info. Flüchtlinge werden 10 km v...   \n",
       "3240        3241  @USER.aha .Mal abwarten kommt bei uns auch .Fi...   \n",
       "3241        3242                                   @USER .So ist es   \n",
       "3242        3243                               @USER .Die warten da   \n",
       "3243        3244  @USER .Das bekommen die gesagt wie sich verhal...   \n",
       "\n",
       "      Sub1_Toxic  Sub2_Engaging  Sub3_FactClaiming  \n",
       "0              0              0                  0  \n",
       "1              0              1                  1  \n",
       "2              1              0                  0  \n",
       "3              0              0                  1  \n",
       "4              0              0                  0  \n",
       "...          ...            ...                ...  \n",
       "3239           0              0                  0  \n",
       "3240           1              0                  1  \n",
       "3241           0              0                  0  \n",
       "3242           0              0                  0  \n",
       "3243           1              0                  0  \n",
       "\n",
       "[3244 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8caf4d-53ab-4657-bc4e-c56c771d3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/emojis_dict.json\") as f:\n",
    "  import json\n",
    "  emoji_dict=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13917f08-066b-4917-94f6-f14373b23bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887ce70978cd49e9b212135f2dc6b443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1238560\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69665ef79c6741c0a542564b8e33b882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "df['user']=df['comment_text'].apply(lambda comment: 1 if \"@USER\" in comment else 0)\n",
    "df['mention']=df['comment_text'].apply(lambda comment: 1 if \"@MEDIUM\" in comment else 0)\n",
    "df['mod']=df['comment_text'].apply(lambda comment: 1 if \"@MODERATOR\" in comment else 0)\n",
    "df['total_length'] = df['comment_text'].apply(len)\n",
    "df['num_words'] = df.comment_text.str.count('\\S+')\n",
    "df['all_caps']= df['comment_text'].apply(lambda comment: sum(1 for c in comment.split() if c.isupper() and c not in [\"@USER\", \"@MEDIUM\", \"@MODERATOR\"]))/df['num_words']\n",
    "df['emoji']=df['comment_text'].apply(lambda comment: len(re.findall(u'[\\U0001f600-\\U0001f650]', comment)))/df['num_words']\n",
    "df['caps']= df['all_caps'].apply(lambda num: 1 if num > 0 else 0 )\n",
    "df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "df['words_vs_unique'] = df['num_unique_words'] / df['num_words']\n",
    "df['num_urls']=df['comment_text'].apply(lambda comment: len(re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', comment)))\n",
    "df['!']=df['comment_text'].apply(lambda comment : comment.count(\"!\"))/df['num_words']\n",
    "df['?']=df['comment_text'].apply(lambda comment : comment.count('?'))/df['num_words']\n",
    "import string\n",
    "c=0\n",
    "def transcribe(text):\n",
    "  global c\n",
    "  new_text=text\n",
    "  for emoji,de_text in emoji_dict.items():\n",
    "    new_text=new_text.replace(emoji,\" \"+de_text+\" \")\n",
    "    if new_text!=text:\n",
    "      c+=1\n",
    "  return new_text\n",
    "df['punc'] = df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))/df['num_words']\n",
    "df[\"text\"]= df['comment_text'].progress_apply(lambda x: transcribe(x))\n",
    "print(c)\n",
    "df[\"text\"]= df['text'].map(lambda x: x.replace(\"@USER\",\"\"))\n",
    "df[\"text\"]= df['text'].map(lambda x: x.replace(\"@MEDIUM\",\"\"))\n",
    "df[\"text\"]= df['text'].map(lambda x: x.replace(\"@MODERATOR\",\"\"))\n",
    "df['text'] = df['text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "# remove IP addresses or user IDs\n",
    "df['text'] = df['text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "# lower uppercase letters\n",
    "df['text'] = df['text'].map(lambda x: str(x).lower())\n",
    "#remove http links in the text\n",
    "df['text'] = df['text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n",
    "#remove all punctuation except for apostrophe (')\n",
    "df['text'] = df['text'].map(lambda x: re.sub('[!\"#$%&\\()*+,-/:;<=>?@[\\\\]^_`{|}~]','',str(x)))\n",
    "df['stop']= df[\"text\"].progress_apply(lambda comment: sum(1 for w in comment.split() if w in stopwords.words('german')))/df['num_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f6b6af3-edb7-4be4-a9d4-d0f7c98d0d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>Sub1_Toxic</th>\n",
       "      <th>Sub2_Engaging</th>\n",
       "      <th>Sub3_FactClaiming</th>\n",
       "      <th>user</th>\n",
       "      <th>mention</th>\n",
       "      <th>mod</th>\n",
       "      <th>total_length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>...</th>\n",
       "      <th>capitals</th>\n",
       "      <th>caps_vs_length</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "      <th>num_urls</th>\n",
       "      <th>!</th>\n",
       "      <th>?</th>\n",
       "      <th>punc</th>\n",
       "      <th>text</th>\n",
       "      <th>stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ziemlich traurig diese Kommentare zu lesen. Ih...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>322</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.040373</td>\n",
       "      <td>48</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>ziemlich traurig diese kommentare zu lesen ihr...</td>\n",
       "      <td>0.509804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sag ich doch, wir befeuern den Klimawandel. Ra...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>22</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>sag ich doch wir befeuern den klimawandel rauc...</td>\n",
       "      <td>0.565217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Schublade auf, Schublade zu. Zu mehr Denkleist...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>12</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>schublade auf schublade zu zu mehr denkleistun...</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Dummerweise haben wir in der EU und in der USA...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.104348</td>\n",
       "      <td>19</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>dummerweise haben wir in der eu und in der usa...</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>\"So lange Gewinnmaximierung Vorrang hat, wird ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>so lange gewinnmaximierung vorrang hat wird si...</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3239</th>\n",
       "      <td>3240</td>\n",
       "      <td>Hier mal eine Info. Flüchtlinge werden 10 km v...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>hier mal eine info flüchtlinge werden 10 km vo...</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3240</th>\n",
       "      <td>3241</td>\n",
       "      <td>@USER.aha .Mal abwarten kommt bei uns auch .Fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>221</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>32</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>aha mal abwarten kommt bei uns auch firmen ent...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3241</th>\n",
       "      <td>3242</td>\n",
       "      <td>@USER .So ist es</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>so ist es</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3242</th>\n",
       "      <td>3243</td>\n",
       "      <td>@USER .Die warten da</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>die warten da</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243</th>\n",
       "      <td>3244</td>\n",
       "      <td>@USER .Das bekommen die gesagt wie sich verhal...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>210</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>31</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>das bekommen die gesagt wie sich verhalten so...</td>\n",
       "      <td>0.527778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3244 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id                                       comment_text  \\\n",
       "0              1  Ziemlich traurig diese Kommentare zu lesen. Ih...   \n",
       "1              2  Sag ich doch, wir befeuern den Klimawandel. Ra...   \n",
       "2              3  Schublade auf, Schublade zu. Zu mehr Denkleist...   \n",
       "3              4  Dummerweise haben wir in der EU und in der USA...   \n",
       "4              5  \"So lange Gewinnmaximierung Vorrang hat, wird ...   \n",
       "...          ...                                                ...   \n",
       "3239        3240  Hier mal eine Info. Flüchtlinge werden 10 km v...   \n",
       "3240        3241  @USER.aha .Mal abwarten kommt bei uns auch .Fi...   \n",
       "3241        3242                                   @USER .So ist es   \n",
       "3242        3243                               @USER .Die warten da   \n",
       "3243        3244  @USER .Das bekommen die gesagt wie sich verhal...   \n",
       "\n",
       "      Sub1_Toxic  Sub2_Engaging  Sub3_FactClaiming  user  mention  mod  \\\n",
       "0              0              0                  0     0        0    0   \n",
       "1              0              1                  1     0        0    0   \n",
       "2              1              0                  0     0        0    0   \n",
       "3              0              0                  1     0        0    0   \n",
       "4              0              0                  0     0        0    0   \n",
       "...          ...            ...                ...   ...      ...  ...   \n",
       "3239           0              0                  0     0        0    0   \n",
       "3240           1              0                  1     1        0    0   \n",
       "3241           0              0                  0     1        0    0   \n",
       "3242           0              0                  0     1        0    0   \n",
       "3243           1              0                  0     1        0    0   \n",
       "\n",
       "      total_length  num_words  ...  capitals  caps_vs_length  \\\n",
       "0              322         51  ...        13        0.040373   \n",
       "1              148         23  ...         6        0.040541   \n",
       "2               79         13  ...         4        0.050633   \n",
       "3              115         21  ...        12        0.104348   \n",
       "4              100         16  ...         4        0.040000   \n",
       "...            ...        ...  ...       ...             ...   \n",
       "3239           130         22  ...         8        0.061538   \n",
       "3240           221         34  ...        13        0.058824   \n",
       "3241            16          4  ...         5        0.312500   \n",
       "3242            20          4  ...         5        0.250000   \n",
       "3243           210         36  ...         9        0.042857   \n",
       "\n",
       "      num_unique_words  words_vs_unique  num_urls    !         ?      punc  \\\n",
       "0                   48         0.941176         0  0.0  0.019608  0.117647   \n",
       "1                   22         0.956522         0  0.0  0.000000  0.173913   \n",
       "2                   12         0.923077         0  0.0  0.000000  0.230769   \n",
       "3                   19         0.904762         0  0.0  0.000000  0.142857   \n",
       "4                   16         1.000000         0  0.0  0.000000  0.250000   \n",
       "...                ...              ...       ...  ...       ...       ...   \n",
       "3239                22         1.000000         0  0.0  0.000000  0.090909   \n",
       "3240                32         0.941176         0  0.0  0.000000  0.205882   \n",
       "3241                 4         1.000000         0  0.0  0.000000  0.500000   \n",
       "3242                 4         1.000000         0  0.0  0.000000  0.500000   \n",
       "3243                31         0.861111         0  0.0  0.000000  0.111111   \n",
       "\n",
       "                                                   text      stop  \n",
       "0     ziemlich traurig diese kommentare zu lesen ihr...  0.509804  \n",
       "1     sag ich doch wir befeuern den klimawandel rauc...  0.565217  \n",
       "2     schublade auf schublade zu zu mehr denkleistun...  0.538462  \n",
       "3     dummerweise haben wir in der eu und in der usa...  0.571429  \n",
       "4     so lange gewinnmaximierung vorrang hat wird si...  0.562500  \n",
       "...                                                 ...       ...  \n",
       "3239  hier mal eine info flüchtlinge werden 10 km vo...  0.454545  \n",
       "3240  aha mal abwarten kommt bei uns auch firmen ent...  0.500000  \n",
       "3241                                          so ist es  0.750000  \n",
       "3242                                      die warten da  0.500000  \n",
       "3243   das bekommen die gesagt wie sich verhalten so...  0.527778  \n",
       "\n",
       "[3244 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "941622a9-d263-4a07-9196-223e03429b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6fef5ee8bc40bb83139a7fee2ff27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f491ba0cfc24bfc85a49000e5111de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "f1 = open(\"../data/positives.json\")\n",
    "f2 = open(\"../data/negatives.json\")\n",
    "positive = json.load(f1)\n",
    "negative = json.load(f2)\n",
    "positive_words = [i[0] for i in positive]\n",
    "positive_scores = [i[1] for i in positive]\n",
    "negative_words = [i[0] for i in negative]\n",
    "negative_scores = [i[1] for i in negative]\n",
    "\n",
    "pos_score = []\n",
    "neg_score = []\n",
    "\n",
    "def find_negative(text):\n",
    "  words = text.split()\n",
    "  count = 0\n",
    "  score = 0\n",
    "  for w in words:\n",
    "    try:\n",
    "      indx = negative_words.index(w.lower())\n",
    "      count+=1\n",
    "      score+= negative_scores[indx]\n",
    "    except:\n",
    "      pass\n",
    "  neg_score.append(abs(score))\n",
    "  return count\n",
    "\n",
    "def find_positive(text):\n",
    "  words = text.split()\n",
    "  count = 0\n",
    "  score = 0\n",
    "  for w in words:\n",
    "    try:\n",
    "      indx = positive_words.index(w.lower())\n",
    "      count+=1\n",
    "      score+= positive_scores[indx]\n",
    "    except:\n",
    "      pass\n",
    "  pos_score.append(abs(score))\n",
    "  return count\n",
    "\n",
    "df[\"num_negative_words\"] = df[\"text\"].progress_apply(lambda x: find_negative(x))\n",
    "df[\"negativity_score\"] = neg_score\n",
    "df[\"num_positive_words\"] = df[\"text\"].progress_apply(lambda x: find_positive(x))\n",
    "df[\"positivity_score\"] = pos_score\n",
    "df['neg']=df[\"negativity_score\"]/df[\"num_negative_words\"]\n",
    "df['pos']=df[\"positivity_score\"]/df[\"num_positive_words\"]\n",
    "df['pos']=df['pos'].fillna(0)\n",
    "df['neg']=df['neg'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23bd6328-cc67-4f87-948d-51a3f3c503b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self._url: https://api.languagetool.org/v2/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2bc28c6884141b393423acecc87b61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageToolPublicAPI('de')\n",
    "match_u=[]\n",
    "def get_errors(text):\n",
    "  global match_u\n",
    "  matches = tool.check(text)\n",
    "  matches=[match for match in matches if match.ruleId not in ['DE_CASE','DE_DASH','DE_DU_UPPER_LOWER','DE_PHRASE_REPETITION','DE_SENTENCE_WHITESPACE','EMAIL','GERMAN_WORD_REPEAT_BEGINNING_RULE',\n",
    " 'GERMAN_WORD_REPEAT_RULE','PLURAL_APOSTROPH','PUNCTUATION_PARAGRAPH_END','UNPAIRED_BRACKETS',\n",
    " 'UPPERCASE_SENTENCE_START']]\n",
    "  match_ui=[match.ruleId for match in matches]\n",
    "  match_u.extend(match_ui)\n",
    "  return len(matches)/len(text)\n",
    "df['error']=df['comment_text'].progress_apply(lambda comment: get_errors(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd20d87f-03ea-424c-9e50-33980e5094de",
   "metadata": {},
   "outputs": [],
   "source": [
    "german = {'present':['nutze ab',\n",
    "                      'nutzt ab',\n",
    "                      'nutzen ab',\n",
    "                      'gehe aus',\n",
    "                      'gehst aus',\n",
    "                      'geht aus',\n",
    "                      'gehen aus',\n",
    "                      'rufe an',\n",
    "                      'rufst an',\n",
    "                      'ruft an',\n",
    "                      'rufen an',\n",
    "                      'rufe an',\n",
    "                      'rufst an',\n",
    "                      'ruft an',\n",
    "                      'rufen an',\n",
    "                      'gehe auf',\n",
    "                      'gehst auf',\n",
    "                      'geht auf',\n",
    "                      'gehen auf',\n",
    "                      'breche zusammen',\n",
    "                      'brichst zusammen',\n",
    "                      'bricht zusammen',\n",
    "                      'brechen zusammen',\n",
    "                      'brecht zusammen',\n",
    "                      'arbeite',\n",
    "                      'arbeitest',\n",
    "                      'arbeitet',\n",
    "                      'arbeiten',\n",
    "                      'bin',\n",
    "                      'bist', \n",
    "                      'ist', \n",
    "                      'sind',#typo\n",
    "                      'seid',#typo\n",
    "                      'sein',\n",
    "                      'komme',\n",
    "                      'kommst',\n",
    "                      'kommt',\n",
    "                      'kommen',\n",
    "                      'steige',\n",
    "                      'steigst',\n",
    "                      'steigt',\n",
    "                      'steigen',\n",
    "                      'bekomme',\n",
    "                      'bekommst',\n",
    "                      'bekommt',\n",
    "                      'bekommen',\n",
    "                      'mache',\n",
    "                      'machst',\n",
    "                      'macht',\n",
    "                      'machen',\n",
    "                      'nutzen',\n",
    "                      'nutze',\n",
    "                      'nutzt',\n",
    "                      'habe',\n",
    "                      'hast',\n",
    "                      'hat',\n",
    "                      'haben',\n",
    "                      'habt',\n",
    "                      'fühle',\n",
    "                      'fühlst',\n",
    "                      'fühlt',\n",
    "                      'fühlen',\n",
    "                      'fuehlst',\n",
    "                      'gewinne',\n",
    "                      'gewinnst',\n",
    "                      'gewinnt',\n",
    "                      'gewinnen',\n",
    "                      'regne',\n",
    "                      'regnest',\n",
    "                      'regnet',\n",
    "                      'regnen',\n",
    "                      'kaufe',\n",
    "                      'kaufst',\n",
    "                      'kauft',\n",
    "                      'kaufen',\n",
    "                      'sterbe',\n",
    "                      'stirbst',\n",
    "                      'stirbt',\n",
    "                      'sterben',\n",
    "                      'sterbt',\n",
    "                      'reise',\n",
    "                      'reist',\n",
    "                      'reisen',\n",
    "                      \"rijs\",#typo\n",
    "                      'ruft',\n",
    "                      'rufst',\n",
    "                      'rufe',\n",
    "                      'rufen',\n",
    "                      'gehe',\n",
    "                      'gehst',\n",
    "                      'geht',\n",
    "                      'gehen',\n",
    "                      'liebe',\n",
    "                      'liebst',\n",
    "                      'liebt',\n",
    "                      'lieben',\n",
    "                      'lebe', \n",
    "                      'lebst', \n",
    "                      'lebt', \n",
    "                      'leben'\n",
    "            ],\n",
    "    'future': \n",
    "            ['werde',\n",
    "              'wirst',\n",
    "              'wird',\n",
    "              'werden',\n",
    "              'werdet',\n",
    "              'werden',\n",
    "              'wirdt',#typo\n",
    "              'werde',\n",
    "              'wirst',\n",
    "              'wird',\n",
    "              'werden',\n",
    "              'werdet',\n",
    "              'werden',\n",
    "              'will',\n",
    "              'willst',\n",
    "              'wollen',\n",
    "              'wollt',\n",
    "              'wollen',\n",
    "              ##konjunktiv ii of mogen meaning to want/would like\n",
    "              'möchte',\n",
    "              'möchtest',\n",
    "              'möchten',\n",
    "              'möchtet',\n",
    "              'hoffe',#hope\n",
    "              'hoffst',\n",
    "              'hofft',\n",
    "              'hoffen',\n",
    "              'plane',\n",
    "              'planst',\n",
    "              'plant',\n",
    "              'plannen',\n",
    "              ],\n",
    "    'uncertainity':\n",
    "                 [#indicative of können\n",
    "                  'kann',\n",
    "                  'kannst',\n",
    "                  'könnt',\n",
    "                  'können',\n",
    "                  \n",
    "                  #konjunktiv ii of können\n",
    "                  'könnte',\n",
    "                  'könntest',\n",
    "                  'könnten',\n",
    "                  'könntet',\n",
    "                  \n",
    "                  #may have some epistemic use, but not typical, as in english ''should' (Nuyts 2000). Both konjunctiv ii and indicative included    \n",
    "                  'sollen',\n",
    "                  'soll',\n",
    "                  'sollst',\n",
    "                  'sollt',\n",
    "                  \n",
    "                  #konjunktiv ii\n",
    "                  'sollte',\n",
    "                  'solltest',\n",
    "                  'sollten',\n",
    "                  'solltet',\n",
    "                  \n",
    "                  #indicative dürfen cannot have epistemic uses, only deontic (Nuyts, 2000)\n",
    "                  #'darf',\n",
    "                  #'darfst',\n",
    "                  #'dürfen',\n",
    "                  #'dürft',\n",
    "                  \n",
    "                  #konjunktiv ii of ¨dürfen\n",
    "                  'dürfte',\n",
    "                  'dürftest',\n",
    "                  'dürften',\n",
    "                  'dürftet',\n",
    "                  \n",
    "                  #mogen 'may' in the indicative has epistemic uses, but not konjunktiv ii (Nuyts 2000)\n",
    "                  'mag',\n",
    "                  'magst',\n",
    "                  'mögen',\n",
    "                  'mögt',\n",
    "                  \n",
    "                  \n",
    "                  ##konjuntiv of werden, i.e. 'would' with epistemic uses: according to informant/coder\n",
    "                  'würde',\n",
    "                  'würdest',\n",
    "                  'würden',\n",
    "                  'würdet',\n",
    "                  \n",
    "                  #konjunktiv ii of müssen has epistemic uses, like 'should' (Mortelmans 2000).\n",
    "                  'müßte',\n",
    "                  'müßtest',\n",
    "                  'müßten',\n",
    "                  'müßtet',\n",
    "                  'müsste',\n",
    "                  'müsstest',\n",
    "                  'müssten',\n",
    "                  'müsstet',\n",
    "                  'unter umständen',\n",
    "                  'annehmbar',#presumably (Nuyts 2000)\n",
    "                  'eventuell',\n",
    "                  'anscheinend',\n",
    "                  'gegebenenfalls',\n",
    "                  'wahrscheinlichkeit',#probability -- informant coder\n",
    "                  'möglich',\n",
    "                  'möglicherweise',\n",
    "                  'offenbar',\n",
    "                  'scheinbar',#seemingly\n",
    "                  #'vielleicht',\n",
    "                  'vermutlich',#presumably (Nuyts 2000)\n",
    "                  'wahrscheinlich',\n",
    "                  'womöglich',\n",
    "                  'wohl',\n",
    "                  'vielleicht',\n",
    "                  'aber',\n",
    "                  'auch',\n",
    "                  'bloß',\n",
    "                  'denn',\n",
    "                  'doch',\n",
    "                  'eigentlich',\n",
    "                  'eben',\n",
    "                  'etwa',\n",
    "                  'einfach',\n",
    "                  'erst',\n",
    "                  'halt',\n",
    "                  'ja',\n",
    "                  'nun',\n",
    "                  'mal',\n",
    "                  'nur',\n",
    "                  'schon',\n",
    "                  'vielleicht',\n",
    "                  'ruhig'\n",
    "                  ],#maybe\n",
    "    \n",
    "    'certainity':\n",
    "                 ['muss',\n",
    "                  'musst',\n",
    "                  'müssen',\n",
    "                  'müsst',\n",
    "                  'muß',\n",
    "                  'mußt',\n",
    "                  'müßen',\n",
    "                  'müßt',\n",
    "                  'auf jeden fall',\n",
    "                  'klipp und klar',\n",
    "                  'aufjedenfall',\n",
    "                  'augenscheinlich',#evidently\n",
    "                  'bestimmt',#certainly\n",
    "                  'definitiv',\n",
    "                  'deutlich',#clearly\n",
    "                  'eindeutig',\n",
    "                  'gewiss',\n",
    "                  'klar',\n",
    "                  'offensichtlich',#obviously\n",
    "                  'jedenfalls',\n",
    "                  'sicher',#certainly (Nuyts 2000)\n",
    "                  'sicherlich',#certainly\n",
    "                  'zweifelsohne',#certainly\n",
    "                  'zweifellos'#certainly\n",
    "                  ],\n",
    "    'mental':      ['nehme an',\n",
    "                    'nimmst an',\n",
    "                    'nimmt an',\n",
    "                    'nehmen an',\n",
    "                    'nehmt an', #assume, but with more clear qualificational use \n",
    "                    'denke',\n",
    "                    'denkst',\n",
    "                    'denkt',\n",
    "                    'denken',\n",
    "                    'glaube',\n",
    "                    'glaubst',\n",
    "                    'glaubt',\n",
    "                    'glauben',# Nuyts 2000\n",
    "                    'meine',\n",
    "                    'meinst',\n",
    "                    'meint',\n",
    "                    'meinen', # to mean, no qualificational use in english, but stronger in german, and menen, too in dutch (Nuyts 2000)\n",
    "                    'vermuten',#to presume (outdated in engish mostly, (Nuyts, 2000))\n",
    "                    'vermute',\n",
    "                    'vermutest',\n",
    "                    'vermutet',\n",
    "                    'rechne',#reckon, qulaificational use, (Nuyts 2000)\n",
    "                    'rechnest',\n",
    "                    'rechnet',\n",
    "                    'rechnen',\n",
    "                    #'sage',#while this can be used epistemically, none of the question frames do so\n",
    "                    #'sagst',# and do use 'to say' in non epistemic ways\n",
    "                    #'sagt',\n",
    "                    #'sagen',\n",
    "                    'erwarte',\n",
    "                    'erwartest',\n",
    "                    'erwartet',\n",
    "                    'erwarten'\n",
    "                  ],\n",
    "     }\n",
    "df['present']= df[\"text\"].apply(lambda comment: sum(1 for w in comment.split() if w in german['present']))/df['num_words']\n",
    "df['future']= df[\"text\"].apply(lambda comment: sum(1 for w in comment.split() if w in german['future']))/df['num_words']\n",
    "df['uncertainity']= df[\"text\"].apply(lambda comment: sum(1 for w in comment.split() if w in german['uncertainity']))/df['num_words']\n",
    "df['certainity']= df[\"text\"].apply(lambda comment: sum(1 for w in comment.split() if w in german['certainity']))/df['num_words']\n",
    "df['mental']= df[\"text\"].apply(lambda comment: sum(1 for w in comment.split() if w in german['mental']))/df['num_words']\n",
    "verbs=pd.read_csv(\"../data/top-german-verbs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26ad48e5-85e5-4aad-8126-add384930dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289bd0cd3bcd4f669da19cd2edeec2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb7e7da4046401e884dd2cf8c7b5d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c76936ad1e4d208f2252ea93b48b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128b97fa79b347c99170266df7cab928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05438dd327454f16b38f00be5ea1d7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c012f617c9f41168ad3618677429048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475cfa66d8f24a87a51b5d3a8b3afab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Infinitiv']= df[\"text\"].progress_apply(lambda comment: sum(1 for w in comment.split() if w in list(verbs['Infinitiv'])))/df['num_words']\n",
    "df['Präsens_ich']= df[\"text\"].progress_apply(lambda comment: sum(1 for w in comment.split() if w in list(verbs['Präsens_ich'])))/df['num_words']\n",
    "df['Präsens_du']= df[\"text\"].progress_apply(lambda comment: sum(1 for w in comment.split() if w in list(verbs['Präsens_du'])))/df['num_words']\n",
    "df['Präsens_er, sie, es']= df[\"text\"].progress_apply(lambda comment: sum(1 for w in comment.split() if w in list(verbs['Präsens_er, sie, es'])))/df['num_words']\n",
    "df['Präteritum_ich']= df[\"text\"].progress_apply(lambda comment: sum(1 for w in comment.split() if w in list(verbs['Präteritum_ich'])))/df['num_words']\n",
    "df['Partizip II']= df[\"text\"].progress_apply(lambda comment: sum(1 for w in comment.split() if w in list(verbs['Partizip II'])))/df['num_words']\n",
    "df['Konjunktiv II_ich']= df[\"text\"].progress_apply(lambda comment: sum(1 for w in comment.split() if w in list(verbs['Konjunktiv II_ich'])))/df['num_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e88ac850-de6f-4f94-8750-979529b68ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>Sub1_Toxic</th>\n",
       "      <th>Sub2_Engaging</th>\n",
       "      <th>Sub3_FactClaiming</th>\n",
       "      <th>user</th>\n",
       "      <th>mention</th>\n",
       "      <th>mod</th>\n",
       "      <th>total_length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>...</th>\n",
       "      <th>uncertainity</th>\n",
       "      <th>certainity</th>\n",
       "      <th>mental</th>\n",
       "      <th>Infinitiv</th>\n",
       "      <th>Präsens_ich</th>\n",
       "      <th>Präsens_du</th>\n",
       "      <th>Präsens_er, sie, es</th>\n",
       "      <th>Präteritum_ich</th>\n",
       "      <th>Partizip II</th>\n",
       "      <th>Konjunktiv II_ich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ziemlich traurig diese Kommentare zu lesen. Ih...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>322</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sag ich doch, wir befeuern den Klimawandel. Ra...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Schublade auf, Schublade zu. Zu mehr Denkleist...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Dummerweise haben wir in der EU und in der USA...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>\"So lange Gewinnmaximierung Vorrang hat, wird ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3239</th>\n",
       "      <td>3240</td>\n",
       "      <td>Hier mal eine Info. Flüchtlinge werden 10 km v...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3240</th>\n",
       "      <td>3241</td>\n",
       "      <td>@USER.aha .Mal abwarten kommt bei uns auch .Fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>221</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3241</th>\n",
       "      <td>3242</td>\n",
       "      <td>@USER .So ist es</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3242</th>\n",
       "      <td>3243</td>\n",
       "      <td>@USER .Die warten da</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243</th>\n",
       "      <td>3244</td>\n",
       "      <td>@USER .Das bekommen die gesagt wie sich verhal...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>210</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3244 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id                                       comment_text  \\\n",
       "0              1  Ziemlich traurig diese Kommentare zu lesen. Ih...   \n",
       "1              2  Sag ich doch, wir befeuern den Klimawandel. Ra...   \n",
       "2              3  Schublade auf, Schublade zu. Zu mehr Denkleist...   \n",
       "3              4  Dummerweise haben wir in der EU und in der USA...   \n",
       "4              5  \"So lange Gewinnmaximierung Vorrang hat, wird ...   \n",
       "...          ...                                                ...   \n",
       "3239        3240  Hier mal eine Info. Flüchtlinge werden 10 km v...   \n",
       "3240        3241  @USER.aha .Mal abwarten kommt bei uns auch .Fi...   \n",
       "3241        3242                                   @USER .So ist es   \n",
       "3242        3243                               @USER .Die warten da   \n",
       "3243        3244  @USER .Das bekommen die gesagt wie sich verhal...   \n",
       "\n",
       "      Sub1_Toxic  Sub2_Engaging  Sub3_FactClaiming  user  mention  mod  \\\n",
       "0              0              0                  0     0        0    0   \n",
       "1              0              1                  1     0        0    0   \n",
       "2              1              0                  0     0        0    0   \n",
       "3              0              0                  1     0        0    0   \n",
       "4              0              0                  0     0        0    0   \n",
       "...          ...            ...                ...   ...      ...  ...   \n",
       "3239           0              0                  0     0        0    0   \n",
       "3240           1              0                  1     1        0    0   \n",
       "3241           0              0                  0     1        0    0   \n",
       "3242           0              0                  0     1        0    0   \n",
       "3243           1              0                  0     1        0    0   \n",
       "\n",
       "      total_length  num_words  ...  uncertainity  certainity  mental  \\\n",
       "0              322         51  ...      0.078431    0.019608     0.0   \n",
       "1              148         23  ...      0.173913    0.000000     0.0   \n",
       "2               79         13  ...      0.076923    0.000000     0.0   \n",
       "3              115         21  ...      0.000000    0.000000     0.0   \n",
       "4              100         16  ...      0.062500    0.000000     0.0   \n",
       "...            ...        ...  ...           ...         ...     ...   \n",
       "3239           130         22  ...      0.045455    0.045455     0.0   \n",
       "3240           221         34  ...      0.117647    0.000000     0.0   \n",
       "3241            16          4  ...      0.000000    0.000000     0.0   \n",
       "3242            20          4  ...      0.000000    0.000000     0.0   \n",
       "3243           210         36  ...      0.027778    0.000000     0.0   \n",
       "\n",
       "      Infinitiv  Präsens_ich  Präsens_du  Präsens_er, sie, es  Präteritum_ich  \\\n",
       "0      0.117647     0.019608         0.0             0.039216             0.0   \n",
       "1      0.043478     0.000000         0.0             0.000000             0.0   \n",
       "2      0.000000     0.000000         0.0             0.076923             0.0   \n",
       "3      0.095238     0.000000         0.0             0.000000             0.0   \n",
       "4      0.062500     0.125000         0.0             0.125000             0.0   \n",
       "...         ...          ...         ...                  ...             ...   \n",
       "3239   0.090909     0.045455         0.0             0.000000             0.0   \n",
       "3240   0.176471     0.029412         0.0             0.088235             0.0   \n",
       "3241   0.000000     0.000000         0.0             0.250000             0.0   \n",
       "3242   0.250000     0.000000         0.0             0.000000             0.0   \n",
       "3243   0.111111     0.000000         0.0             0.083333             0.0   \n",
       "\n",
       "      Partizip II  Konjunktiv II_ich  \n",
       "0        0.019608                0.0  \n",
       "1        0.000000                0.0  \n",
       "2        0.000000                0.0  \n",
       "3        0.000000                0.0  \n",
       "4        0.000000                0.0  \n",
       "...           ...                ...  \n",
       "3239     0.000000                0.0  \n",
       "3240     0.029412                0.0  \n",
       "3241     0.000000                0.0  \n",
       "3242     0.000000                0.0  \n",
       "3243     0.222222                0.0  \n",
       "\n",
       "[3244 rows x 42 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.fillna(\" \")\n",
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c4f7f9b-b7a5-431b-841e-d9679b429c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63faf41edc1400c95cfda1f0a370b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "import readability\n",
    "def get_readability(text):\n",
    "  try:\n",
    "    results = readability.getmeasures(text, lang='de')\n",
    "  except:\n",
    "    print(\"error\")\n",
    "    return 0\n",
    "  return results[\"readability grades\"][\"DaleChallIndex\"]\n",
    "df[\"readability\"]=df[\"comment_text\"].progress_apply(lambda x: get_readability(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51d6deaa-261d-4aa0-8ec9-d89f9d0c5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/train_ft.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9de839-4c72-4302-b0d1-86dc8417816f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
