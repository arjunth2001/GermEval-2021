{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a7fd38-6883-4cb3-a425-b659230b547f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>Sub1_Toxic</th>\n",
       "      <th>Sub2_Engaging</th>\n",
       "      <th>Sub3_FactClaiming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ziemlich traurig diese Kommentare zu lesen. Ih...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sag ich doch, wir befeuern den Klimawandel. Ra...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Schublade auf, Schublade zu. Zu mehr Denkleist...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Dummerweise haben wir in der EU und in der USA...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>\"So lange Gewinnmaximierung Vorrang hat, wird ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3239</th>\n",
       "      <td>3240</td>\n",
       "      <td>Hier mal eine Info. Flüchtlinge werden 10 km v...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3240</th>\n",
       "      <td>3241</td>\n",
       "      <td>@USER.aha .Mal abwarten kommt bei uns auch .Fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3241</th>\n",
       "      <td>3242</td>\n",
       "      <td>@USER .So ist es</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3242</th>\n",
       "      <td>3243</td>\n",
       "      <td>@USER .Die warten da</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243</th>\n",
       "      <td>3244</td>\n",
       "      <td>@USER .Das bekommen die gesagt wie sich verhal...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3244 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id                                       comment_text  \\\n",
       "0              1  Ziemlich traurig diese Kommentare zu lesen. Ih...   \n",
       "1              2  Sag ich doch, wir befeuern den Klimawandel. Ra...   \n",
       "2              3  Schublade auf, Schublade zu. Zu mehr Denkleist...   \n",
       "3              4  Dummerweise haben wir in der EU und in der USA...   \n",
       "4              5  \"So lange Gewinnmaximierung Vorrang hat, wird ...   \n",
       "...          ...                                                ...   \n",
       "3239        3240  Hier mal eine Info. Flüchtlinge werden 10 km v...   \n",
       "3240        3241  @USER.aha .Mal abwarten kommt bei uns auch .Fi...   \n",
       "3241        3242                                   @USER .So ist es   \n",
       "3242        3243                               @USER .Die warten da   \n",
       "3243        3244  @USER .Das bekommen die gesagt wie sich verhal...   \n",
       "\n",
       "      Sub1_Toxic  Sub2_Engaging  Sub3_FactClaiming  \n",
       "0              0              0                  0  \n",
       "1              0              1                  1  \n",
       "2              1              0                  0  \n",
       "3              0              0                  1  \n",
       "4              0              0                  0  \n",
       "...          ...            ...                ...  \n",
       "3239           0              0                  0  \n",
       "3240           1              0                  1  \n",
       "3241           0              0                  0  \n",
       "3242           0              0                  0  \n",
       "3243           1              0                  0  \n",
       "\n",
       "[3244 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import tqdm.notebook as tq\n",
    "from googletrans import Translator\n",
    "import json\n",
    "import string\n",
    "tqdm.pandas()\n",
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b9805b2-484d-421a-8d2d-875080ea5b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user']=df['comment_text'].apply(lambda comment: 1 if \"@USER\" in comment else 0)\n",
    "df['mention']=df['comment_text'].apply(lambda comment: 1 if \"@MEDIUM\" in comment else 0)\n",
    "df['mod']=df['comment_text'].apply(lambda comment: 1 if \"@MODERATOR\" in comment else 0)\n",
    "df[\"text\"]= df['comment_text'].map(lambda x: x.replace(\"@USER\",\"\"))\n",
    "df[\"text\"]= df['text'].map(lambda x: x.replace(\"@MEDIUM\",\"\"))\n",
    "df[\"text\"]= df['text'].map(lambda x: x.replace(\"@MODERATOR\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671c765-d1a0-4858-bbcf-c87d86a9960f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bad3af95c064fc69f10d4385d279ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855130182ccc4652a435d86809fd8eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/arjunth2001/miniconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/emojis_dict.json\") as f:\n",
    "  emoji_dict=json.load(f)\n",
    "\n",
    "df['user']=df['comment_text'].apply(lambda comment: 1 if \"@USER\" in comment else 0)\n",
    "df['mention']=df['comment_text'].apply(lambda comment: 1 if \"@MEDIUM\" in comment else 0)\n",
    "df['mod']=df['comment_text'].apply(lambda comment: 1 if \"@MODERATOR\" in comment else 0)\n",
    "df['total_length'] = df['comment_text'].apply(len)\n",
    "df['num_words'] = df.comment_text.str.count('\\S+')\n",
    "df['all_caps']= df['comment_text'].apply(lambda comment: sum(1 for c in comment.split() if c.isupper() and c not in [\"@USER\", \"@MEDIUM\", \"@MODERATOR\"]))/df['num_words']\n",
    "df['emoji']=df['comment_text'].apply(lambda comment: len(re.findall(u'[\\U0001f600-\\U0001f650]', comment)))/df['num_words']\n",
    "df['caps']= df['all_caps'].apply(lambda num: 1 if num > 0 else 0 )\n",
    "df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "df['words_vs_unique'] = df['num_unique_words'] / df['num_words']\n",
    "df['num_urls']=df['comment_text'].apply(lambda comment: len(re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', comment)))\n",
    "df['!']=df['comment_text'].apply(lambda comment : comment.count(\"!\"))/df['num_words']\n",
    "df['?']=df['comment_text'].apply(lambda comment : comment.count('?'))/df['num_words']\n",
    "c=0\n",
    "def transcribe(text):\n",
    "  global c\n",
    "  new_text=text\n",
    "  for emoji,de_text in emoji_dict.items():\n",
    "    new_text=text.replace(emoji,\" \"+de_text+\" \")\n",
    "    if new_text!=text:\n",
    "      c+=1\n",
    "  return new_text\n",
    "df['punc'] = df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))/df['num_words']\n",
    "df[\"text\"]= df['comment_text'].progress_apply(lambda x: transcribe(x))\n",
    "print(c)\n",
    "translator = Translator()\n",
    "for idx in tq.tqdm(range(0,len(df))):\n",
    "  try:\n",
    "    text=df[\"text\"][idx]\n",
    "    if text == 0:\n",
    "      df[\"text\"][idx]=\"\"\n",
    "      continue\n",
    "    en_text=translator.translate(text,src=\"de\",dest=\"en\").text\n",
    "    de_text=translator.translate(en_text,src=\"en\",dest=\"de\").text\n",
    "    df[\"text\"][idx]=de_text\n",
    "  except:\n",
    "    print(\"error\")\n",
    "df[\"text\"]= df['comment_text'].map(lambda x: x.replace(\"@USER\",\"\"))\n",
    "df[\"text\"]= df['text'].map(lambda x: x.replace(\"@MEDIUM\",\"\"))\n",
    "df[\"text\"]= df['text'].map(lambda x: x.replace(\"@MODERATOR\",\"\"))\n",
    "df['text'] = df['text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "# remove IP addresses or user IDs\n",
    "df['text'] = df['text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "# lower uppercase letters\n",
    "df['text'] = df['text'].map(lambda x: str(x).lower())\n",
    "#remove http links in the text\n",
    "df['text'] = df['text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n",
    "#remove all punctuation except for apostrophe (')\n",
    "df['text'] = df['text'].map(lambda x: re.sub('[!\"#$%&\\()*+,-/:;<=>?@[\\\\]^_`{|}~]','',str(x)))\n",
    "df['stop']= df[\"text\"].progress_apply(lambda comment: sum(1 for w in comment.split() if w in stopwords.words('german')))/df['num_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc550cd-a336-4e27-8e07-69af5621baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(\"../data/positives.json\")\n",
    "f2 = open(\"../data/negatives.json\")\n",
    "positive = json.load(f1)\n",
    "negative = json.load(f2)\n",
    "positive_words = [i[0] for i in positive]\n",
    "positive_scores = [i[1] for i in positive]\n",
    "negative_words = [i[0] for i in negative]\n",
    "negative_scores = [i[1] for i in negative]\n",
    "\n",
    "pos_score = []\n",
    "neg_score = []\n",
    "\n",
    "def find_negative(text):\n",
    "  words = text.split()\n",
    "  count = 0\n",
    "  score = 0\n",
    "  for w in words:\n",
    "    try:\n",
    "      indx = negative_words.index(w.lower())\n",
    "      count+=1\n",
    "      score+= negative_scores[indx]\n",
    "    except:\n",
    "      pass\n",
    "  neg_score.append(abs(score))\n",
    "  return count\n",
    "\n",
    "def find_positive(text):\n",
    "  words = text.split()\n",
    "  count = 0\n",
    "  score = 0\n",
    "  for w in words:\n",
    "    try:\n",
    "      indx = positive_words.index(w.lower())\n",
    "      count+=1\n",
    "      score+= positive_scores[indx]\n",
    "    except:\n",
    "      pass\n",
    "  pos_score.append(abs(score))\n",
    "  return count\n",
    "\n",
    "df[\"num_negative_words\"] = df[\"text\"].progress_apply(lambda x: find_negative(x))\n",
    "df[\"negativity_score\"] = neg_score\n",
    "df[\"num_positive_words\"] = df[\"text\"].progress_apply(lambda x: find_positive(x))\n",
    "df[\"positivity_score\"] = pos_score\n",
    "df['neg']=df[\"negativity_score\"]/df[\"num_negative_words\"]\n",
    "df['pos']=df[\"positivity_score\"]/df[\"num_positive_words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65920bb-b2de-4deb-955c-d8bef628efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageToolPublicAPI('de')\n",
    "match_u=[]\n",
    "def get_errors(text):\n",
    "  global match_u\n",
    "  matches = tool.check(text)\n",
    "  matches=[match for match in matches if match.ruleId not in ['DE_CASE','DE_DASH','DE_DU_UPPER_LOWER','DE_PHRASE_REPETITION','DE_SENTENCE_WHITESPACE','EMAIL','GERMAN_WORD_REPEAT_BEGINNING_RULE',\n",
    " 'GERMAN_WORD_REPEAT_RULE','PLURAL_APOSTROPH','PUNCTUATION_PARAGRAPH_END','UNPAIRED_BRACKETS',\n",
    " 'UPPERCASE_SENTENCE_START']]\n",
    "  match_ui=[match.ruleId for match in matches]\n",
    "  match_u.extend(match_ui)\n",
    "  return len(matches)/len(text)\n",
    "df['error']=df['comment_text'].progress_apply(lambda comment: get_errors(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da3ffbe-3c60-437a-97f2-307103527307",
   "metadata": {},
   "outputs": [],
   "source": [
    "german = {'present':['nutze ab',\n",
    "                      'nutzt ab',\n",
    "                      'nutzen ab',\n",
    "                      'gehe aus',\n",
    "                      'gehst aus',\n",
    "                      'geht aus',\n",
    "                      'gehen aus',\n",
    "                      'rufe an',\n",
    "                      'rufst an',\n",
    "                      'ruft an',\n",
    "                      'rufen an',\n",
    "                      'rufe an',\n",
    "                      'rufst an',\n",
    "                      'ruft an',\n",
    "                      'rufen an',\n",
    "                      'gehe auf',\n",
    "                      'gehst auf',\n",
    "                      'geht auf',\n",
    "                      'gehen auf',\n",
    "                      'breche zusammen',\n",
    "                      'brichst zusammen',\n",
    "                      'bricht zusammen',\n",
    "                      'brechen zusammen',\n",
    "                      'brecht zusammen',\n",
    "                      'arbeite',\n",
    "                      'arbeitest',\n",
    "                      'arbeitet',\n",
    "                      'arbeiten',\n",
    "                      'bin',\n",
    "                      'bist', \n",
    "                      'ist', \n",
    "                      'sind',#typo\n",
    "                      'seid',#typo\n",
    "                      'sein',\n",
    "                      'komme',\n",
    "                      'kommst',\n",
    "                      'kommt',\n",
    "                      'kommen',\n",
    "                      'steige',\n",
    "                      'steigst',\n",
    "                      'steigt',\n",
    "                      'steigen',\n",
    "                      'bekomme',\n",
    "                      'bekommst',\n",
    "                      'bekommt',\n",
    "                      'bekommen',\n",
    "                      'mache',\n",
    "                      'machst',\n",
    "                      'macht',\n",
    "                      'machen',\n",
    "                      'nutzen',\n",
    "                      'nutze',\n",
    "                      'nutzt',\n",
    "                      'habe',\n",
    "                      'hast',\n",
    "                      'hat',\n",
    "                      'haben',\n",
    "                      'habt',\n",
    "                      'fühle',\n",
    "                      'fühlst',\n",
    "                      'fühlt',\n",
    "                      'fühlen',\n",
    "                      'fuehlst',\n",
    "                      'gewinne',\n",
    "                      'gewinnst',\n",
    "                      'gewinnt',\n",
    "                      'gewinnen',\n",
    "                      'regne',\n",
    "                      'regnest',\n",
    "                      'regnet',\n",
    "                      'regnen',\n",
    "                      'kaufe',\n",
    "                      'kaufst',\n",
    "                      'kauft',\n",
    "                      'kaufen',\n",
    "                      'sterbe',\n",
    "                      'stirbst',\n",
    "                      'stirbt',\n",
    "                      'sterben',\n",
    "                      'sterbt',\n",
    "                      'reise',\n",
    "                      'reist',\n",
    "                      'reisen',\n",
    "                      \"rijs\",#typo\n",
    "                      'ruft',\n",
    "                      'rufst',\n",
    "                      'rufe',\n",
    "                      'rufen',\n",
    "                      'gehe',\n",
    "                      'gehst',\n",
    "                      'geht',\n",
    "                      'gehen',\n",
    "                      'liebe',\n",
    "                      'liebst',\n",
    "                      'liebt',\n",
    "                      'lieben',\n",
    "                      'lebe', \n",
    "                      'lebst', \n",
    "                      'lebt', \n",
    "                      'leben'\n",
    "            ],\n",
    "    'future': \n",
    "            ['werde',\n",
    "              'wirst',\n",
    "              'wird',\n",
    "              'werden',\n",
    "              'werdet',\n",
    "              'werden',\n",
    "              'wirdt',#typo\n",
    "              'werde',\n",
    "              'wirst',\n",
    "              'wird',\n",
    "              'werden',\n",
    "              'werdet',\n",
    "              'werden',\n",
    "              'will',\n",
    "              'willst',\n",
    "              'wollen',\n",
    "              'wollt',\n",
    "              'wollen',\n",
    "              ##konjunktiv ii of mogen meaning to want/would like\n",
    "              'möchte',\n",
    "              'möchtest',\n",
    "              'möchten',\n",
    "              'möchtet',\n",
    "              'hoffe',#hope\n",
    "              'hoffst',\n",
    "              'hofft',\n",
    "              'hoffen',\n",
    "              'plane',\n",
    "              'planst',\n",
    "              'plant',\n",
    "              'plannen',\n",
    "              ],\n",
    "    'uncertainity':\n",
    "                 [#indicative of können\n",
    "                  'kann',\n",
    "                  'kannst',\n",
    "                  'könnt',\n",
    "                  'können',\n",
    "                  \n",
    "                  #konjunktiv ii of können\n",
    "                  'könnte',\n",
    "                  'könntest',\n",
    "                  'könnten',\n",
    "                  'könntet',\n",
    "                  \n",
    "                  #may have some epistemic use, but not typical, as in english ''should' (Nuyts 2000). Both konjunctiv ii and indicative included    \n",
    "                  'sollen',\n",
    "                  'soll',\n",
    "                  'sollst',\n",
    "                  'sollt',\n",
    "                  \n",
    "                  #konjunktiv ii\n",
    "                  'sollte',\n",
    "                  'solltest',\n",
    "                  'sollten',\n",
    "                  'solltet',\n",
    "                  \n",
    "                  #indicative dürfen cannot have epistemic uses, only deontic (Nuyts, 2000)\n",
    "                  #'darf',\n",
    "                  #'darfst',\n",
    "                  #'dürfen',\n",
    "                  #'dürft',\n",
    "                  \n",
    "                  #konjunktiv ii of ¨dürfen\n",
    "                  'dürfte',\n",
    "                  'dürftest',\n",
    "                  'dürften',\n",
    "                  'dürftet',\n",
    "                  \n",
    "                  #mogen 'may' in the indicative has epistemic uses, but not konjunktiv ii (Nuyts 2000)\n",
    "                  'mag',\n",
    "                  'magst',\n",
    "                  'mögen',\n",
    "                  'mögt',\n",
    "                  \n",
    "                  \n",
    "                  ##konjuntiv of werden, i.e. 'would' with epistemic uses: according to informant/coder\n",
    "                  'würde',\n",
    "                  'würdest',\n",
    "                  'würden',\n",
    "                  'würdet',\n",
    "                  \n",
    "                  #konjunktiv ii of müssen has epistemic uses, like 'should' (Mortelmans 2000).\n",
    "                  'müßte',\n",
    "                  'müßtest',\n",
    "                  'müßten',\n",
    "                  'müßtet',\n",
    "                  'müsste',\n",
    "                  'müsstest',\n",
    "                  'müssten',\n",
    "                  'müsstet',\n",
    "                  'unter umständen',\n",
    "                  'annehmbar',#presumably (Nuyts 2000)\n",
    "                  'eventuell',\n",
    "                  'anscheinend',\n",
    "                  'gegebenenfalls',\n",
    "                  'wahrscheinlichkeit',#probability -- informant coder\n",
    "                  'möglich',\n",
    "                  'möglicherweise',\n",
    "                  'offenbar',\n",
    "                  'scheinbar',#seemingly\n",
    "                  #'vielleicht',\n",
    "                  'vermutlich',#presumably (Nuyts 2000)\n",
    "                  'wahrscheinlich',\n",
    "                  'womöglich',\n",
    "                  'wohl',\n",
    "                  'vielleicht',\n",
    "                  'aber',\n",
    "                  'auch',\n",
    "                  'bloß',\n",
    "                  'denn',\n",
    "                  'doch',\n",
    "                  'eigentlich',\n",
    "                  'eben',\n",
    "                  'etwa',\n",
    "                  'einfach',\n",
    "                  'erst',\n",
    "                  'halt',\n",
    "                  'ja',\n",
    "                  'nun',\n",
    "                  'mal',\n",
    "                  'nur',\n",
    "                  'schon',\n",
    "                  'vielleicht',\n",
    "                  'ruhig'\n",
    "                  ],#maybe\n",
    "    \n",
    "    'certainity':\n",
    "                 ['muss',\n",
    "                  'musst',\n",
    "                  'müssen',\n",
    "                  'müsst',\n",
    "                  'muß',\n",
    "                  'mußt',\n",
    "                  'müßen',\n",
    "                  'müßt',\n",
    "                  'auf jeden fall',\n",
    "                  'klipp und klar',\n",
    "                  'aufjedenfall',\n",
    "                  'augenscheinlich',#evidently\n",
    "                  'bestimmt',#certainly\n",
    "                  'definitiv',\n",
    "                  'deutlich',#clearly\n",
    "                  'eindeutig',\n",
    "                  'gewiss',\n",
    "                  'klar',\n",
    "                  'offensichtlich',#obviously\n",
    "                  'jedenfalls',\n",
    "                  'sicher',#certainly (Nuyts 2000)\n",
    "                  'sicherlich',#certainly\n",
    "                  'zweifelsohne',#certainly\n",
    "                  'zweifellos'#certainly\n",
    "                  ],\n",
    "    'mental':      ['nehme an',\n",
    "                    'nimmst an',\n",
    "                    'nimmt an',\n",
    "                    'nehmen an',\n",
    "                    'nehmt an', #assume, but with more clear qualificational use \n",
    "                    'denke',\n",
    "                    'denkst',\n",
    "                    'denkt',\n",
    "                    'denken',\n",
    "                    'glaube',\n",
    "                    'glaubst',\n",
    "                    'glaubt',\n",
    "                    'glauben',# Nuyts 2000\n",
    "                    'meine',\n",
    "                    'meinst',\n",
    "                    'meint',\n",
    "                    'meinen', # to mean, no qualificational use in english, but stronger in german, and menen, too in dutch (Nuyts 2000)\n",
    "                    'vermuten',#to presume (outdated in engish mostly, (Nuyts, 2000))\n",
    "                    'vermute',\n",
    "                    'vermutest',\n",
    "                    'vermutet',\n",
    "                    'rechne',#reckon, qulaificational use, (Nuyts 2000)\n",
    "                    'rechnest',\n",
    "                    'rechnet',\n",
    "                    'rechnen',\n",
    "                    #'sage',#while this can be used epistemically, none of the question frames do so\n",
    "                    #'sagst',# and do use 'to say' in non epistemic ways\n",
    "                    #'sagt',\n",
    "                    #'sagen',\n",
    "                    'erwarte',\n",
    "                    'erwartest',\n",
    "                    'erwartet',\n",
    "                    'erwarten'\n",
    "                  ],\n",
    "     }\n",
    "df['present']= df[\"text\"].apply(lambda comment: sum(1 for w in comment.split() if w in german['present']))/df['num_words']\n",
    "df['future']= df[\"text\"].apply(lambda comment: sum(1 for w in comment.split() if w in german['future']))/df['num_words']\n",
    "df['uncertainity']= df[\"text\"].apply(lambda comment: sum(1 for w in comment.split() if w in german['uncertainity']))/df['num_words']\n",
    "df['certainity']= df[\"text\"].apply(lambda comment: sum(1 for w in comment.split() if w in german['certainity']))/df['num_words']\n",
    "df['mental']= df[\"text\"].apply(lambda comment: sum(1 for w in comment.split() if w in german['mental']))/df['num_words']\n",
    "verbs=pd.read_csv(\"../data/top-german-verbs.csv\")\n",
    "df['Infinitiv']= df[\"text\"].progress_apply(lambda comment: sum(1 for w in comment.split() if w in list(verbs['Infinitiv'])))/df['num_words']\n",
    "df['Präsens_ich']= df[\"text\"].progress_apply(lambda comment: sum(1 for w in comment.split() if w in list(verbs['Präsens_ich'])))/df['num_words']\n",
    "df['Präsens_du']= df[\"text\"].progress_apply(lambda comment: sum(1 for w in comment.split() if w in list(verbs['Präsens_du'])))/df['num_words']\n",
    "df['Präsens_er, sie, es']= df[\"text\"].progress_apply(lambda comment: sum(1 for w in comment.split() if w in list(verbs['Präsens_er, sie, es'])))/df['num_words']\n",
    "df['Präteritum_ich']= df[\"text\"].progress_apply(lambda comment: sum(1 for w in comment.split() if w in list(verbs['Präteritum_ich'])))/df['num_words']\n",
    "df['Partizip II']= df[\"text\"].progress_apply(lambda comment: sum(1 for w in comment.split() if w in list(verbs['Partizip II'])))/df['num_words']\n",
    "df['Konjunktiv II_ich']= df[\"text\"].progress_apply(lambda comment: sum(1 for w in comment.split() if w in list(verbs['Konjunktiv II_ich'])))/df['num_words']\n",
    "df.text.fillna(\" \")\n",
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91627356-b6d9-44a2-a870-9bd6b3cf605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import readability\n",
    "def get_readability(text):\n",
    "  try:\n",
    "    results = readability.getmeasures(text, lang='de')\n",
    "  except:\n",
    "    print(\"error\")\n",
    "    return 0\n",
    "  return results[\"readability grades\"][\"DaleChallIndex\"]\n",
    "df[\"readability\"]=df[\"text\"].progress_apply(lambda x: get_readability(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13908d64-f541-4b19-895c-53b96c7b1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/itrain_aug.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b9c69-4ab1-4783-8ae0-53a19ca3cd2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
